
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Comprehensive Guide on Feature Selection &#8212; Feature Selection and Extraction Methods</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=fb9458d3" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/theme.css?v=a243ae73" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_sources/classical_methods';</script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/copybutton_funcs.js?v=776a791e"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/language_data.js?v=d4673a71"></script>
    <script src="../_static/searchtools.js?v=d24376de"></script>
    <script src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/bootstrap.js?v=7583a70d"></script>
    <script src="../_static/scripts/fontawesome.js?v=9b125980"></script>
    <script src="../_static/scripts/pydata-sphinx-theme.js?v=a1eb5d9f"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Feature Selection and Extraction Methods - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Feature Selection and Extraction Methods - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    1. Introduction (intro.md)
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../featureImportance.html">featureImportance.ipynb</a></li>


<li class="toctree-l1"><a class="reference internal" href="../classical_methods.html"><strong>Comprehensive Guide on Feature Selection</strong></a></li>







<li class="toctree-l1"><a class="reference internal" href="../boosting_methods.html">3. Boosting Methods (boosting_methods.md)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neural_network_methods.html">4. Neural Network Methods (neural_network_methods.md)</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F_sources/classical_methods.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/_sources/classical_methods.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Comprehensive Guide on Feature Selection</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>Comprehensive Guide on Feature Selection</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-feature-selection"><strong>1. Introduction to Feature Selection</strong> <a class="anchor" id="1"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection"><strong>Feature selection</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-selecting-features"><strong>Advantages of selecting features</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-techniques"><strong>Feature Selection – Techniques</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-source-miro-medium-com">Image source : miro.medium.com</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#filter-methods"><strong>Filter Methods</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapper-methods"><strong>Wrapper Methods</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedded-methods"><strong>Embedded Methods</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>2. Filter Methods</strong> <a class="anchor" id="2"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-methods"><strong>2.1 Basic methods</strong> <a class="anchor" id="2.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-constant-features"><strong>2.1.1 Remove constant features</strong> <a class="anchor" id="2.1.1"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important"><strong>Important</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-variance-threshold-from-sklearn"><strong>Using variance threshold from sklearn</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-quasi-constant-features"><strong>2.1.2 Remove quasi-constant features</strong> <a class="anchor" id="2.1.2"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-source-towardsdatascience">Image source : towardsdatascience</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#removing-quasi-constant-features"><strong>Removing quasi-constant features</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Using variance threshold from sklearn</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-selection-methods"><strong>2.2 Univariate selection methods</strong> <a class="anchor" id="2.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selectkbest"><strong>2.2.1 SelectKBest</strong> <a class="anchor" id="2.2.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selectpercentile"><strong>2.2.2 SelectPercentile</strong> <a class="anchor" id="2.2.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#important-information"><strong>Important information</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-with-sparse-data"><strong>Feature selection with sparse data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warning"><strong>Warning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#information-gain"><strong>2.3 Information Gain</strong> <a class="anchor" id="2.3"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-info-classif"><strong>mutual_info_classif</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-info-regression"><strong>mutual_info_regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-score-chi-square-implementation"><strong>2.4 Fisher Score (chi-square implementation)</strong> <a class="anchor" id="2.4"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#anova-f-value-for-feature-selection"><strong>2.5 ANOVA F-value For Feature Selection</strong> <a class="anchor" id="2.5"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f-value">F-value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-matrix-with-heatmap"><strong>2.6 Correlation-Matrix with Heatmap</strong> <a class="anchor" id="2.6"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relief-based-feature-selection"><strong>2.7 Relief-based feature selection</strong> <a class="anchor" id="2.7"></a></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>3. Wrapper Methods</strong> <a class="anchor" id="3"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-selection"><strong>3.1 Forward Selection</strong> <a class="anchor" id="3.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-elimination"><strong>3.2 Backward Elimination</strong> <a class="anchor" id="3.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exhaustive-feature-selection"><strong>3.3 Exhaustive Feature Selection</strong> <a class="anchor" id="3.3"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination"><strong>3.4 Recursive Feature elimination</strong> <a class="anchor" id="3.4"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination-with-cross-validation"><strong>3.5 Recursive Feature Elimination with Cross-Validation</strong> <a class="anchor" id="3.5"></a></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>4. Embedded Methods</strong> <a class="anchor" id="4"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-source-analyticsvidhya">Image source : AnalyticsVidhya</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression"><strong>4.1 LASSO Regression</strong><a class="anchor" id="4.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-importance"><strong>4.2 Random Forest Importance</strong><a class="anchor" id="4.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance"><strong>Feature Importance</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-choose-the-right-feature-selection-method"><strong>5. How to choose the right feature selection method</strong> <a class="anchor" id="5"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-input-numerical-output"><strong>Numerical Input, Numerical Output</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-input-categorical-output"><strong>Numerical Input, Categorical Output</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-input-numerical-output"><strong>Categorical Input, Numerical Output</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-input-categorical-output"><strong>Categorical Input, Categorical Output</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#tips-and-tricks-for-feature-selection"><strong>6. Tips and Tricks for Feature Selection</strong> <a class="anchor" id="6"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-statistics"><strong>Correlation Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-method"><strong>Selection Method</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transform-variables"><strong>Transform Variables</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-best-method"><strong>What Is the Best Method?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-ways-of-feature-selection"><strong>4 best ways of Feature Selection</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references"><strong>7. References</strong> <a class="anchor" id="7"></a></a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### 2. Classical Methods (classical_methods.ipynb)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="anchor" id="0"></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="comprehensive-guide-on-feature-selection">
<h1><strong>Comprehensive Guide on Feature Selection</strong><a class="headerlink" href="#comprehensive-guide-on-feature-selection" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection">reference</a></p>
<p><strong>Feature Selection</strong> is the process of selecting optimal number of features from a larger set of features. There are several advantages of this feature selection process and also there are various techniques available for this feature selection process. In this kernel, we will look at these advantages and various techniques for feature selection.</p>
<p>So, let’s get started.</p>
<p>This kernel is based on Soledad Galli’s course - <a class="reference external" href="https://www.udemy.com/course/feature-selection-for-machine-learning/">Feature Selection for Machine Learning</a></p>
<p>She had done a fabulous job in her above course wherein she had put all the major feature selection techniques together at one place. I have adapted code and instructions from her course in this kernel. I like to congratulate her for her excellent work.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction-to-feature-selection">
<h1><strong>1. Introduction to Feature Selection</strong> <a class="anchor" id="1"></a><a class="headerlink" href="#introduction-to-feature-selection" title="Link to this heading">#</a></h1>
<!-- [Table of Contents](#0.1) -->
<section id="feature-selection">
<h2><strong>Feature selection</strong><a class="headerlink" href="#feature-selection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Feature selection</strong> or <strong>variable selection</strong> is the process of selecting a subset of relevant features or variables from the total features of a level in a data set to build machine learning algorithms.</p></li>
</ul>
</section>
<section id="advantages-of-selecting-features">
<h2><strong>Advantages of selecting features</strong><a class="headerlink" href="#advantages-of-selecting-features" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>There are various advantages of feature selection process. These are as follows:-</p>
<ol class="arabic simple">
<li><p>Improved accuracy</p></li>
<li><p>Simple models are easier to interpret.</p></li>
<li><p>Shorter training times</p></li>
<li><p>Enhanced generalization by reducing Overfitting</p></li>
<li><p>Easier to implement by software developers</p></li>
<li><p>Reduced risk of data errors by model use</p></li>
<li><p>Variable redundancy</p></li>
<li><p>Bad learning behaviour in high dimensional spaces</p></li>
</ol>
</li>
</ul>
</section>
<section id="feature-selection-techniques">
<h2><strong>Feature Selection – Techniques</strong><a class="headerlink" href="#feature-selection-techniques" title="Link to this heading">#</a></h2>
<ul>
<li><p>Feature selection techniques are categorized into 3 typers. These are as follows:-</p>
<ol class="arabic simple">
<li><p>Filter methods</p></li>
<li><p>Wrapper methods</p></li>
<li><p>Embedded methods</p></li>
</ol>
<p><img alt="Feature Selection Methods" src="https://miro.medium.com/v2/resize:fit:1400/1*9h2qPmOJonbCdthfeVkuyg.jpeg" /></p>
</li>
</ul>
<section id="image-source-miro-medium-com">
<h3>Image source : <a class="reference external" href="http://miro.medium.com">miro.medium.com</a><a class="headerlink" href="#image-source-miro-medium-com" title="Link to this heading">#</a></h3>
</section>
</section>
<section id="filter-methods">
<h2><strong>Filter Methods</strong><a class="headerlink" href="#filter-methods" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Filter methods consists of various techniques as given below:-</p>
<ol class="arabic simple">
<li><p>Basic methods</p></li>
<li><p>Univariate methods</p></li>
<li><p>Information gain</p></li>
<li><p>Fischer score</p></li>
<li><p>Correlation Matrix with Heatmap</p></li>
</ol>
</li>
</ul>
</section>
<section id="wrapper-methods">
<h2><strong>Wrapper Methods</strong><a class="headerlink" href="#wrapper-methods" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Wrapper methods consists of the following techniques:-</p>
<ol class="arabic simple">
<li><p>Forward Selection</p></li>
<li><p>Backward Elimination</p></li>
<li><p>Exhaustive Feature Selection</p></li>
<li><p>Recursive Feature Elimination</p></li>
<li><p>Recursive Feature Elimination with Cross-Validation</p></li>
</ol>
</li>
</ul>
</section>
<section id="embedded-methods">
<h2><strong>Embedded Methods</strong><a class="headerlink" href="#embedded-methods" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Embedded methods consists of the following techniques:-</p>
<ol class="arabic simple">
<li><p>LASSO</p></li>
<li><p>RIDGE</p></li>
<li><p>Tree Importance</p></li>
</ol>
</li>
<li><p>Now, we will discuss these methods in detail.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><strong>2. Filter Methods</strong> <a class="anchor" id="2"></a><a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<!-- [Table of Contents](#0.1) -->
<ul class="simple">
<li><p>Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The characteristics of these methods are as follows:-</p>
<ul>
<li><p>These methods rely on the characteristics of the data (feature characteristics)</p></li>
<li><p>They do not use machine learning algorithms.</p></li>
<li><p>These are model agnostic.</p></li>
<li><p>They tend to be less computationally expensive.</p></li>
<li><p>They usually give lower prediction performance than wrapper methods.</p></li>
<li><p>They are very well suited for a quick screen and removal of irrelevant features.</p></li>
</ul>
</li>
<li><p>Filter methods consists of various techniques as given below:-</p>
<ul>
<li><p>2.1.    Basic methods</p></li>
<li><p>2.2.    Univariate feature selection</p></li>
<li><p>2.3.    Information gain</p></li>
<li><p>2.4.    Fischer score</p></li>
<li><p>2.5.     ANOVA F-Value for Feature Selection</p></li>
<li><p>2.6.    Correlation Matrix with Heatmap</p></li>
</ul>
</li>
<li><p>Filter methods can be explained with the help of following graphic:</p></li>
</ul>
<center>
<img src = "https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537552825/Image3_fqsh79.png"  width = "700"/>
</center>
### Image source : AnalyticsVidhya<section id="basic-methods">
<h2><strong>2.1 Basic methods</strong> <a class="anchor" id="2.1"></a><a class="headerlink" href="#basic-methods" title="Link to this heading">#</a></h2>
<!-- [Table of Contents](#0.1) -->
<ul class="simple">
<li><p>Under basic methods, we remove constant and quasi-constant features.</p></li>
</ul>
</section>
<section id="remove-constant-features">
<h2><strong>2.1.1 Remove constant features</strong> <a class="anchor" id="2.1.1"></a><a class="headerlink" href="#remove-constant-features" title="Link to this heading">#</a></h2>
<!-- [Table of Contents](#0.1) -->
<ul class="simple">
<li><p>Constant features are those that show the same value, just one value, for all the observations of the dataset. This is, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.</p></li>
<li><p>Identifying and removing constant features, is an easy first step towards feature selection and more easily interpretable machine learning models. To identify constant features, we can use the VarianceThreshold function from sklearn.</p></li>
<li><p>I will demonstrate how to identify constant features using the Santander Customer Satisfaction dataset from Kaggle.</p></li>
<li><p>Source :</p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/feature_selection.html">https://scikit-learn.org/stable/modules/feature_selection.html</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This Python 3 environment comes with many helpful analytics libraries installed</span>
<span class="c1"># It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python</span>
<span class="c1"># For example, here&#39;s several helpful packages to load in </span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> <span class="c1"># linear algebra</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> <span class="c1"># data processing, CSV file I/O (e.g. pd.read_csv)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Input data files are available in the &quot;../input/&quot; directory.</span>
<span class="c1"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="k">for</span> <span class="n">dirname</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">filenames</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">walk</span><span class="p">(</span><span class="s1">&#39;/kaggle/input&#39;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">filename</span><span class="p">))</span>

<span class="c1"># Any results you write to the current directory are saved as output.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ignore warnings</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import the Santander customer satisfaction dataset from Kaggle</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/santander-customer-satisfaction/train.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">35000</span><span class="p">)</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/santander-customer-satisfaction/test.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">15000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># import the Santander customer satisfaction dataset from Kaggle</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/santander-customer-satisfaction/train.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">35000</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/santander-customer-satisfaction/test.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">15000</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:1026,</span> in <span class="ni">read_csv</span><span class="nt">(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)</span>
<span class="g g-Whitespace">   </span><span class="mi">1013</span> <span class="n">kwds_defaults</span> <span class="o">=</span> <span class="n">_refine_defaults_read</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1014</span>     <span class="n">dialect</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1015</span>     <span class="n">delimiter</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1022</span>     <span class="n">dtype_backend</span><span class="o">=</span><span class="n">dtype_backend</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1023</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1024</span> <span class="n">kwds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwds_defaults</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1026</span> <span class="k">return</span> <span class="n">_read</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="n">kwds</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:620,</span> in <span class="ni">_read</span><span class="nt">(filepath_or_buffer, kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">617</span> <span class="n">_validate_names</span><span class="p">(</span><span class="n">kwds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;names&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">619</span> <span class="c1"># Create the parser.</span>
<span class="ne">--&gt; </span><span class="mi">620</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">TextFileReader</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">622</span> <span class="k">if</span> <span class="n">chunksize</span> <span class="ow">or</span> <span class="n">iterator</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">623</span>     <span class="k">return</span> <span class="n">parser</span>

<span class="nn">File ~\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:1620,</span> in <span class="ni">TextFileReader.__init__</span><span class="nt">(self, f, engine, **kwds)</span>
<span class="g g-Whitespace">   </span><span class="mi">1617</span>     <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">[</span><span class="s2">&quot;has_index_names&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwds</span><span class="p">[</span><span class="s2">&quot;has_index_names&quot;</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">1619</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="p">:</span> <span class="n">IOHandles</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="ne">-&gt; </span><span class="mi">1620</span> <span class="bp">self</span><span class="o">.</span><span class="n">_engine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_engine</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\Lib\site-packages\pandas\io\parsers\readers.py:1880,</span> in <span class="ni">TextFileReader._make_engine</span><span class="nt">(self, f, engine)</span>
<span class="g g-Whitespace">   </span><span class="mi">1878</span>     <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1879</span>         <span class="n">mode</span> <span class="o">+=</span> <span class="s2">&quot;b&quot;</span>
<span class="ne">-&gt; </span><span class="mi">1880</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="n">get_handle</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1881</span>     <span class="n">f</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1882</span>     <span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1883</span>     <span class="n">encoding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoding&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1884</span>     <span class="n">compression</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;compression&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1885</span>     <span class="n">memory_map</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;memory_map&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1886</span>     <span class="n">is_text</span><span class="o">=</span><span class="n">is_text</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1887</span>     <span class="n">errors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoding_errors&quot;</span><span class="p">,</span> <span class="s2">&quot;strict&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1888</span>     <span class="n">storage_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;storage_options&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1889</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1890</span> <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">   </span><span class="mi">1891</span> <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="o">.</span><span class="n">handle</span>

<span class="nn">File ~\anaconda3\Lib\site-packages\pandas\io\common.py:873,</span> in <span class="ni">get_handle</span><span class="nt">(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">868</span> <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">869</span>     <span class="c1"># Check whether the filename is to be opened in binary mode.</span>
<span class="g g-Whitespace">    </span><span class="mi">870</span>     <span class="c1"># Binary mode does not support &#39;encoding&#39; and &#39;newline&#39;.</span>
<span class="g g-Whitespace">    </span><span class="mi">871</span>     <span class="k">if</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">encoding</span> <span class="ow">and</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">872</span>         <span class="c1"># Encoding</span>
<span class="ne">--&gt; </span><span class="mi">873</span>         <span class="n">handle</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">874</span>             <span class="n">handle</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">875</span>             <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">876</span>             <span class="n">encoding</span><span class="o">=</span><span class="n">ioargs</span><span class="o">.</span><span class="n">encoding</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">877</span>             <span class="n">errors</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">878</span>             <span class="n">newline</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">879</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">880</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">881</span>         <span class="c1"># Binary mode</span>
<span class="g g-Whitespace">    </span><span class="mi">882</span>         <span class="n">handle</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;/kaggle/input/santander-customer-satisfaction/train.csv&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID</th>
      <th>var3</th>
      <th>var15</th>
      <th>imp_ent_var16_ult1</th>
      <th>imp_op_var39_comer_ult1</th>
      <th>imp_op_var39_comer_ult3</th>
      <th>imp_op_var40_comer_ult1</th>
      <th>imp_op_var40_comer_ult3</th>
      <th>imp_op_var40_efect_ult1</th>
      <th>imp_op_var40_efect_ult3</th>
      <th>...</th>
      <th>saldo_medio_var33_hace2</th>
      <th>saldo_medio_var33_hace3</th>
      <th>saldo_medio_var33_ult1</th>
      <th>saldo_medio_var33_ult3</th>
      <th>saldo_medio_var44_hace2</th>
      <th>saldo_medio_var44_hace3</th>
      <th>saldo_medio_var44_ult1</th>
      <th>saldo_medio_var44_ult3</th>
      <th>var38</th>
      <th>TARGET</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>23</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>39205.170000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>2</td>
      <td>34</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>49278.030000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>2</td>
      <td>23</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>67333.770000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8</td>
      <td>2</td>
      <td>37</td>
      <td>0.0</td>
      <td>195.00</td>
      <td>195.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>64007.970000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10</td>
      <td>2</td>
      <td>39</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>117310.979016</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>34995</th>
      <td>69974</td>
      <td>2</td>
      <td>48</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>98642.430000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>34996</th>
      <td>69976</td>
      <td>2</td>
      <td>65</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>128930.100000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>34997</th>
      <td>69977</td>
      <td>2</td>
      <td>23</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>117310.979016</td>
      <td>0</td>
    </tr>
    <tr>
      <th>34998</th>
      <td>69981</td>
      <td>2</td>
      <td>28</td>
      <td>0.0</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>114747.060000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>34999</th>
      <td>69982</td>
      <td>2</td>
      <td>23</td>
      <td>45.0</td>
      <td>25.71</td>
      <td>42.27</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>117310.979016</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>35000 rows × 371 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># drop TARGET label from X_train</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;TARGET&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check shape of training and test sets</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((35000, 370), (15000, 370))
</pre></div>
</div>
</div>
</div>
<section id="important">
<h3><strong>Important</strong><a class="headerlink" href="#important" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In all feature selection procedures, it is good practice to select the features by examining only the training set. This is done to avoid overfitting.</p></li>
</ul>
</section>
<section id="using-variance-threshold-from-sklearn">
<h3><strong>Using variance threshold from sklearn</strong><a class="headerlink" href="#using-variance-threshold-from-sklearn" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.</p></li>
</ul>
<p>As remember, variance ralation is
<font size = "4">
$<span class="math notranslate nohighlight">\(\sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2\)</span>$
</font></p>
<p>As a simple example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    var(x)=p(1-p)
</pre></div>
</div>
<p>so we can select using the threshold .8 * (1 - .8):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">sel</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="p">(</span><span class="mf">.8</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">.8</span><span class="p">)))</span>
<span class="n">sel</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0, 1],
       [1, 0],
       [0, 0],
       [1, 1],
       [1, 0],
       [1, 1]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># using sklearn variancethreshold to find constant features</span>

<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="n">sel</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># fit finds the features with zero variance</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VarianceThreshold(threshold=0)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get_support is a boolean vector that indicates which features are retained</span>
<span class="c1"># if we sum over get_support, we get the number of features that are not constant</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>319
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># alternate way of finding non-constant features</span>
<span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">()])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>319
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># print the constant features</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">([</span>
        <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">()]</span>
    <span class="p">]))</span>

<span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">()]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>51
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;ind_var2_0&#39;,
 &#39;ind_var2&#39;,
 &#39;ind_var18_0&#39;,
 &#39;ind_var18&#39;,
 &#39;ind_var27_0&#39;,
 &#39;ind_var28_0&#39;,
 &#39;ind_var28&#39;,
 &#39;ind_var27&#39;,
 &#39;ind_var34_0&#39;,
 &#39;ind_var34&#39;,
 &#39;ind_var41&#39;,
 &#39;ind_var46_0&#39;,
 &#39;ind_var46&#39;,
 &#39;num_var18_0&#39;,
 &#39;num_var18&#39;,
 &#39;num_var27_0&#39;,
 &#39;num_var28_0&#39;,
 &#39;num_var28&#39;,
 &#39;num_var27&#39;,
 &#39;num_var34_0&#39;,
 &#39;num_var34&#39;,
 &#39;num_var41&#39;,
 &#39;num_var46_0&#39;,
 &#39;num_var46&#39;,
 &#39;saldo_var18&#39;,
 &#39;saldo_var28&#39;,
 &#39;saldo_var27&#39;,
 &#39;saldo_var34&#39;,
 &#39;saldo_var41&#39;,
 &#39;saldo_var46&#39;,
 &#39;delta_imp_amort_var18_1y3&#39;,
 &#39;delta_imp_amort_var34_1y3&#39;,
 &#39;imp_amort_var18_hace3&#39;,
 &#39;imp_amort_var18_ult1&#39;,
 &#39;imp_amort_var34_hace3&#39;,
 &#39;imp_amort_var34_ult1&#39;,
 &#39;imp_reemb_var13_hace3&#39;,
 &#39;imp_reemb_var17_hace3&#39;,
 &#39;imp_reemb_var33_hace3&#39;,
 &#39;imp_trasp_var17_out_hace3&#39;,
 &#39;imp_trasp_var33_out_hace3&#39;,
 &#39;num_var2_0_ult1&#39;,
 &#39;num_var2_ult1&#39;,
 &#39;num_reemb_var13_hace3&#39;,
 &#39;num_reemb_var17_hace3&#39;,
 &#39;num_reemb_var33_hace3&#39;,
 &#39;num_trasp_var17_out_hace3&#39;,
 &#39;num_trasp_var33_out_hace3&#39;,
 &#39;saldo_var2_ult1&#39;,
 &#39;saldo_medio_var13_medio_hace3&#39;,
 &#39;saldo_medio_var29_hace3&#39;]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that there are 51 columns / variables that are constant. This means that 51 variables show the same value, just one value, for all the observations of the training set.</p></li>
</ul>
<ul class="simple">
<li><p>We then use the transform function to reduce the training and testing sets.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can then drop these columns from the train and test sets</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check the shape of training and test set</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((35000, 319), (15000, 319))
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see how by removing constant features, we managed to reduced the feature space quite a bit.</p></li>
</ul>
</section>
</section>
<section id="remove-quasi-constant-features">
<h2><strong>2.1.2 Remove quasi-constant features</strong> <a class="anchor" id="2.1.2"></a><a class="headerlink" href="#remove-quasi-constant-features" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>Quasi-constant features are those that show the same value for the great majority of the observations of the dataset. In general, these features provide little if any information that allows a machine learning model to discriminate or predict a target. But there can be exceptions. So we should be careful when removing these type of features. Identifying and removing quasi-constant features, is an easy first step towards feature selection and more easily interpretable machine learning models.</p></li>
</ul>
<center>
    <img src = "https://miro.medium.com/v2/resize:fit:828/format:webp/1*jpL5e5XDN4y4kyhV1np_Pg.png" width = "500" />
</center>
<section id="image-source-towardsdatascience">
<h3>Image source : towardsdatascience<a class="headerlink" href="#image-source-towardsdatascience" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>To identify quasi-constant features, we can once again use the VarianceThreshold function from sklearn.</p></li>
<li><p>Here I will demonstrate how to identify quasi-constant features using the Santander Customer Satisfaction dataset.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import the Santander customer satisfaction dataset from Kaggle</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/santander-customer-satisfaction/train.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">35000</span><span class="p">)</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/santander-customer-satisfaction/test.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">15000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># drop TARGET label from X_train</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;TARGET&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check shape of training and test sets</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((35000, 370), (15000, 370))
</pre></div>
</div>
</div>
</div>
</section>
<section id="removing-quasi-constant-features">
<h3><strong>Removing quasi-constant features</strong><a class="headerlink" href="#removing-quasi-constant-features" title="Link to this heading">#</a></h3>
<section id="id2">
<h4><strong>Using variance threshold from sklearn</strong><a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.</p></li>
<li><p>Here, I will change the default threshold to remove almost / quasi-constant features.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># 0.01 indicates 99% of observations approximately</span>

<span class="n">sel</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># fit finds the features with low variance</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VarianceThreshold(threshold=0.01)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get_support is a boolean vector that indicates which features </span>
<span class="c1"># are retained. If we sum over get_support, we get the number</span>
<span class="c1"># of features that are not quasi-constant</span>
<span class="nb">sum</span><span class="p">(</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>263
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># alternative way of doing the above operation:</span>
<span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">()])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>263
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># finally we can print the quasi-constant features</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">([</span>
        <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">()]</span>
    <span class="p">]))</span>

<span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">()]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>107
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;ind_var1&#39;,
 &#39;ind_var2_0&#39;,
 &#39;ind_var2&#39;,
 &#39;ind_var6_0&#39;,
 &#39;ind_var6&#39;,
 &#39;ind_var13_largo&#39;,
 &#39;ind_var13_medio_0&#39;,
 &#39;ind_var13_medio&#39;,
 &#39;ind_var14&#39;,
 &#39;ind_var17_0&#39;,
 &#39;ind_var17&#39;,
 &#39;ind_var18_0&#39;,
 &#39;ind_var18&#39;,
 &#39;ind_var19&#39;,
 &#39;ind_var20_0&#39;,
 &#39;ind_var20&#39;,
 &#39;ind_var27_0&#39;,
 &#39;ind_var28_0&#39;,
 &#39;ind_var28&#39;,
 &#39;ind_var27&#39;,
 &#39;ind_var29_0&#39;,
 &#39;ind_var29&#39;,
 &#39;ind_var30_0&#39;,
 &#39;ind_var31_0&#39;,
 &#39;ind_var31&#39;,
 &#39;ind_var32_cte&#39;,
 &#39;ind_var32_0&#39;,
 &#39;ind_var32&#39;,
 &#39;ind_var33_0&#39;,
 &#39;ind_var33&#39;,
 &#39;ind_var34_0&#39;,
 &#39;ind_var34&#39;,
 &#39;ind_var40&#39;,
 &#39;ind_var41&#39;,
 &#39;ind_var39&#39;,
 &#39;ind_var44_0&#39;,
 &#39;ind_var44&#39;,
 &#39;ind_var46_0&#39;,
 &#39;ind_var46&#39;,
 &#39;num_var6_0&#39;,
 &#39;num_var6&#39;,
 &#39;num_var13_medio_0&#39;,
 &#39;num_var13_medio&#39;,
 &#39;num_var18_0&#39;,
 &#39;num_var18&#39;,
 &#39;num_op_var40_hace3&#39;,
 &#39;num_var27_0&#39;,
 &#39;num_var28_0&#39;,
 &#39;num_var28&#39;,
 &#39;num_var27&#39;,
 &#39;num_var29_0&#39;,
 &#39;num_var29&#39;,
 &#39;num_var33&#39;,
 &#39;num_var34_0&#39;,
 &#39;num_var34&#39;,
 &#39;num_var41&#39;,
 &#39;num_var46_0&#39;,
 &#39;num_var46&#39;,
 &#39;saldo_var18&#39;,
 &#39;saldo_var28&#39;,
 &#39;saldo_var27&#39;,
 &#39;saldo_var34&#39;,
 &#39;saldo_var41&#39;,
 &#39;saldo_var46&#39;,
 &#39;delta_imp_amort_var18_1y3&#39;,
 &#39;delta_imp_amort_var34_1y3&#39;,
 &#39;delta_imp_aport_var33_1y3&#39;,
 &#39;delta_num_aport_var33_1y3&#39;,
 &#39;imp_amort_var18_hace3&#39;,
 &#39;imp_amort_var18_ult1&#39;,
 &#39;imp_amort_var34_hace3&#39;,
 &#39;imp_amort_var34_ult1&#39;,
 &#39;imp_reemb_var13_hace3&#39;,
 &#39;imp_reemb_var17_hace3&#39;,
 &#39;imp_reemb_var33_hace3&#39;,
 &#39;imp_trasp_var17_out_hace3&#39;,
 &#39;imp_trasp_var33_out_hace3&#39;,
 &#39;ind_var7_emit_ult1&#39;,
 &#39;ind_var7_recib_ult1&#39;,
 &#39;num_var2_0_ult1&#39;,
 &#39;num_var2_ult1&#39;,
 &#39;num_aport_var33_hace3&#39;,
 &#39;num_aport_var33_ult1&#39;,
 &#39;num_var7_emit_ult1&#39;,
 &#39;num_meses_var13_medio_ult3&#39;,
 &#39;num_meses_var17_ult3&#39;,
 &#39;num_meses_var29_ult3&#39;,
 &#39;num_meses_var33_ult3&#39;,
 &#39;num_meses_var44_ult3&#39;,
 &#39;num_reemb_var13_hace3&#39;,
 &#39;num_reemb_var13_ult1&#39;,
 &#39;num_reemb_var17_hace3&#39;,
 &#39;num_reemb_var17_ult1&#39;,
 &#39;num_reemb_var33_hace3&#39;,
 &#39;num_reemb_var33_ult1&#39;,
 &#39;num_trasp_var17_in_hace3&#39;,
 &#39;num_trasp_var17_in_ult1&#39;,
 &#39;num_trasp_var17_out_hace3&#39;,
 &#39;num_trasp_var17_out_ult1&#39;,
 &#39;num_trasp_var33_in_hace3&#39;,
 &#39;num_trasp_var33_in_ult1&#39;,
 &#39;num_trasp_var33_out_hace3&#39;,
 &#39;num_trasp_var33_out_ult1&#39;,
 &#39;num_venta_var44_hace3&#39;,
 &#39;saldo_var2_ult1&#39;,
 &#39;saldo_medio_var13_medio_hace3&#39;,
 &#39;saldo_medio_var29_hace3&#39;]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that 107 columns / variables are almost constant. This means that 107 variables show predominantly one value for ~99% the observations of the training set.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># percentage of observations showing each of the different values</span>
<span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;ind_var31&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    0.996286
1    0.003714
Name: ind_var31, dtype: float64
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that &gt; 99% of the observations show one value, 0. Therefore, this feature is almost constant.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can then remove the features from training and test set</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check the shape of training and test set</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((35000, 263), (15000, 263))
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>By removing constant and quasi-constant features, we reduced the feature space from 370 to 263. We can see that more than 100 features were removed from the present dataset.</p></li>
</ul>
</section>
</section>
</section>
<section id="univariate-selection-methods">
<h2><strong>2.2 Univariate selection methods</strong> <a class="anchor" id="2.2"></a><a class="headerlink" href="#univariate-selection-methods" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>Univariate feature selection methods works by selecting the best features based on univariate statistical tests like ANOVA. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method.</p></li>
<li><p>Univariate feature selection <strong>examines each feature individually</strong> to determine the strength of the relationship of the feature with the response variable (target). These methods are simple to run and understand and are in general particularly good for gaining a better understanding of data (but not necessarily for optimizing the feature set for better generalization).</p></li>
<li><p>The methods based on F-test estimate the degree of linear dependency between two random variables. They assume a linear relationship between the feature and the target. These methods also assume that the variables follow a Gaussian distribution.</p></li>
<li><p>There are 4 methods that fall under this category :-</p>
<ol class="arabic simple">
<li><p>SelectKBest</p></li>
<li><p>SelectPercentile</p></li>
<li><p>SelectFpr, SelectFdr, or family wise error SelectFwe</p></li>
<li><p>GenericUnivariateSelection</p></li>
</ol>
</li>
</ul>
<p>Source : <a class="reference external" href="https://scikit-learn.org/stable/modules/feature_selection.html">https://scikit-learn.org/stable/modules/feature_selection.html</a></p>
<ul class="simple">
<li><p>Here, I will limit the discussion to SelectKBest and SelectPercentile, because these two are most commonly used in practice.</p></li>
</ul>
</section>
<section id="selectkbest">
<h2><strong>2.2.1 SelectKBest</strong> <a class="anchor" id="2.2.1"></a><a class="headerlink" href="#selectkbest" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>This method select features according to the k highest scores.</p></li>
<li><p>For instance, we can perform a chi-square test to the samples to retrieve only the two best features from iris dataset as follows:</p></li>
</ul>
<p>Source : <a class="reference external" href="https://scikit-learn.org/stable/modules/feature_selection.html">https://scikit-learn.org/stable/modules/feature_selection.html</a></p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">chi2</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(150, 4)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># select the two best features</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(150, 2)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Thus, we have selected the two best features from the iris dataset.</p></li>
</ul>
</section>
<section id="selectpercentile">
<h2><strong>2.2.2 SelectPercentile</strong> <a class="anchor" id="2.2.2"></a><a class="headerlink" href="#selectpercentile" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>Select features according to a percentile of the highest scores.</p></li>
</ul>
<p>Source : <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectPercentile</span><span class="p">,</span> <span class="n">chi2</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1797, 64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># now select features based on top 10 percentile</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">SelectPercentile</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1797, 7)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that only 7 features lie on the top 10 percentile and hence we select them accordingly.</p></li>
</ul>
</section>
<section id="important-information">
<h2><strong>Important information</strong><a class="headerlink" href="#important-information" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>These objects take as input a scoring function that returns univariate scores and p-values (or only scores for <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest">SelectKBest</a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile">SelectPercentile</a>:</p></li>
<li><p>For regression tasks: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression">f_regression</a>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression">mutual_info_regression</a></p></li>
<li><p>For classification tasks: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2">chi2</a>,
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif">f_classif</a>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif">mutual_info_classif</a></p></li>
</ul>
<p>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</p>
</section>
<section id="feature-selection-with-sparse-data">
<h2><strong>Feature selection with sparse data</strong><a class="headerlink" href="#feature-selection-with-sparse-data" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>If you use sparse data (i.e. data represented as sparse matrices), <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2">chi2</a>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression">mutual_info_regression</a>, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif">mutual_info_classif</a> will deal with the data without making it dense.</p></li>
</ul>
<p>Source : <a class="reference external" href="https://scikit-learn.org/stable/modules/feature_selection.html">https://scikit-learn.org/stable/modules/feature_selection.html</a></p>
</section>
<section id="warning">
<h2><strong>Warning</strong><a class="headerlink" href="#warning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Beware not to use a regression scoring function with a classification problem, you will get useless results.</p></li>
</ul>
</section>
<section id="information-gain">
<h2><strong>2.3 Information Gain</strong> <a class="anchor" id="2.3"></a><a class="headerlink" href="#information-gain" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p><strong>Information gain</strong> or <strong>mutual information</strong> measures how much information the presence/absence of a feature contributes to making the correct prediction on the target.</p></li>
<li><p>In terms of  <a class="reference external" href="https://en.wikipedia.org/wiki/Mutual_information">wikipedia</a>:</p>
<ul>
<li><p>Mutual information measures the information that X and Y share: It measures how much knowing one of these        variables reduces uncertainty about the other. For example, if X and Y are independent, then knowing X does        not give any information about Y and vice versa, so their mutual information is zero. At the other extreme, if      X is a deterministic function of Y and Y is a deterministic function of X then all information conveyed by X        is shared with Y: knowing X determines the value of Y and vice versa. As a result, in this case the mutual          information is the same as the uncertainty contained in Y (or X) alone, namely the entropy of Y (or X).            Moreover, this mutual information is the same as the entropy of X and as the entropy of Y. (A very special          case of this is when X and Y are the same random variable.)</p></li>
</ul>
</li>
</ul>
<center>  
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/1280px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png" alt="drawing" width="500"/>
</center>
<p>Venn diagram showing additive and subtractive relationships of various information measures associated with correlated variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.[1] The area contained by either circles is the joint entropy <span class="math notranslate nohighlight">\(H(X,Y)\)</span>. The circle on the left (red and violet) is the individual entropy <span class="math notranslate nohighlight">\(H(X)\)</span>, with the red being the conditional entropy <span class="math notranslate nohighlight">\(H(X\mid Y)\)</span>. The circle on the right (blue and violet) is <span class="math notranslate nohighlight">\(H(Y)\)</span>, with the blue being <span class="math notranslate nohighlight">\(H(Y\mid X)\)</span>. The violet is the mutual information <span class="math notranslate nohighlight">\(I(X;Y)\)</span>.</p>
<p>The entropy of a random variable is the average level of “information” or “uncertainty” inherent to the variable’s possible outcomes. Given a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> is distributed according to <span class="math notranslate nohighlight">\(X \to [0,1]\)</span>, the entropy is</p>
<p><span class="math notranslate nohighlight">\({H} (X):=-\sum _{x\in {\mathcal {X}}}p(x)\log p(x)\)</span></p>
<center>
    <img src= "https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/450px-Binary_entropy_plot.svg.png" width="300" />
</center>
<p>Also, the conditional entropy quantifies the amount of information needed to describe the outcome of a random variable <span class="math notranslate nohighlight">\(Y\)</span> given that the value of another random variable <span class="math notranslate nohighlight">\(X\)</span> is known. Here, information is measured in shannons, nats, or hartleys. The entropy of <span class="math notranslate nohighlight">\(Y\)</span> conditioned on <span class="math notranslate nohighlight">\(X\)</span> is written as
<span class="math notranslate nohighlight">\({H} (Y|X)\)</span> and is computed as</p>
<p><span class="math notranslate nohighlight">\({H} (Y|X)\ =-\sum _{x\in {\mathcal {X}},y\in {\mathcal {Y}}}p(x,y)\log {\frac {p(x,y)}{p(x)}}\)</span></p>
<p>Now the mutual information can be computed as</p>
<p><strong><span class="math notranslate nohighlight">\({I} (X;Y)=\sum _{y\in {\mathcal {Y}}}\sum _{x\in {\mathcal {X}}}{P_{(X,Y)}(x,y)\log \left({\frac {P_{(X,Y)}(x,y)}{P_{X}(x)\,P_{Y}(y)}}\right)}=H(Y)-H(Y\mid X)\)</span></strong></p>
<p>(Proof:<a class="reference external" href="https://wikimedia.org/api/rest_v1/media/math/render/svg/9c1877f635a7f1d6ba6a6cc0e65a9b7b1fec7b43">https://wikimedia.org/api/rest_v1/media/math/render/svg/9c1877f635a7f1d6ba6a6cc0e65a9b7b1fec7b43</a>)</p>
</section>
<section id="mutual-info-classif">
<h2><strong>mutual_info_classif</strong><a class="headerlink" href="#mutual-info-classif" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>It estimates mutual information for a discrete target variable.</p></li>
<li><p>Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</p></li>
<li><p>This function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.</p></li>
<li><p>It can be used for univariate features selection.</p></li>
<li><p>Source :</p></li>
</ul>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif</a></p>
</section>
<section id="mutual-info-regression">
<h2><strong>mutual_info_regression</strong><a class="headerlink" href="#mutual-info-regression" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Estimate mutual information for a continuous target variable.</p></li>
<li><p>Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</p></li>
<li><p>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.</p></li>
<li><p>It can be used for univariate features selection</p></li>
<li><p>Source :</p></li>
</ul>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression</a></p>
</section>
<section id="fisher-score-chi-square-implementation">
<h2><strong>2.4 Fisher Score (chi-square implementation)</strong> <a class="anchor" id="2.4"></a><a class="headerlink" href="#fisher-score-chi-square-implementation" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<p>Fisher score aims to find a feature subset, such that selected features maximize the distances between data points in different classes while minimizing the distances between data points in the same class.
In particular, given a training dataset <span class="math notranslate nohighlight">\(X\in R_{mn}\)</span>
with respect to c different classes, the Fisher score of the ith feature is computed by</p>
<p><span class="math notranslate nohighlight">\(F(f_i)=\frac{S_b(f_i)}{\sum_{k=1}^c S_t^{(k)}(f_i)}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(S_b(f_i) = \sum_{k=1}^c n_k(\mu_i^{(k)}-\mu_i)^2\)</span> is the between class scatter of the ith feature, <span class="math notranslate nohighlight">\(n_k\)</span> is the number of samples in the kth class, <span class="math notranslate nohighlight">\(\mu_i^{(k)}\)</span> is the mean of the ith feature in the kth class, <span class="math notranslate nohighlight">\(\mu_i\)</span> is the mean of the ith feature in <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(S_t^{(k)}(f_i) = \sum_{j=1}^{n_k}(x_{ij}^{(k)}-\mu_i^{(k)})^2\)</span> is the within class scatter matrix of the ith feature with respect to the kth class, and <span class="math notranslate nohighlight">\(x_{ij}^{(k)}\)</span> denotes the value of the ith feature for jth sample in kth class.</p>
<p>(ref: <a class="reference external" href="https://sci-hub.se/https://www.sciencedirect.com/science/article/abs/pii/S002002552100832X">https://sci-hub.se/https://www.sciencedirect.com/science/article/abs/pii/S002002552100832X</a>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">skfeature</span><span class="o">-</span><span class="n">chappers</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x79bad6ef25f8&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/skfeature-chappers/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x79bad6ef2518&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/skfeature-chappers/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x79bad6ef2470&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/skfeature-chappers/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x79bad6ef2400&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/skfeature-chappers/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x79bad6ef23c8&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/skfeature-chappers/</span>
<span class=" -Color -Color-Red">ERROR: Could not find a version that satisfies the requirement skfeature-chappers (from versions: none)</span>
<span class=" -Color -Color-Red">ERROR: No matching distribution found for skfeature-chappers</span>
Note: you may need to restart the kernel to use updated packages.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">skfeature.function.similarity_based</span> <span class="kn">import</span> <span class="n">fisher_score</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">data</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">fisher_score</span><span class="o">.</span><span class="n">fisher_score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;rank&#39;</span><span class="p">)</span> <span class="c1">#returns rank directly instead of fisher score. so no need for feature_ranking</span>
<span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">8</span><span class="n">d224f829b05</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span> <span class="nn">skfeature.function.similarity_based</span> <span class="kn">import</span> <span class="n">fisher_score</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">db</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">target</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;skfeature&#39;
</pre></div>
</div>
</div>
</div>
<p>It is the <strong>chi-square implementation</strong> in scikit-learn. It computes chi-squared stats between each non-negative feature and class.</p>
<ul class="simple">
<li><p>This score should be used to evaluate categorical variables in a classification task. It compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">chi2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load Data</span>
<span class="c1"># load iris data</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># create features and target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># convert to categorical data by converting data to integers</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compare Chi-Squared Statistics</span>
<span class="c1"># select two features with highest chi-squared statistics</span>
<span class="n">chi2_selector</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_kbest</span> <span class="o">=</span> <span class="n">chi2_selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View results</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original number of features:&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Reduced number of features:&#39;</span><span class="p">,</span> <span class="n">X_kbest</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original number of features: 4
Reduced number of features: 2
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that the above code helps us to select the 2 best features based on Fisher score.</p></li>
</ul>
</section>
<section id="anova-f-value-for-feature-selection">
<h2><strong>2.5 ANOVA F-value For Feature Selection</strong>  <a class="anchor" id="2.5"></a><a class="headerlink" href="#anova-f-value-for-feature-selection" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<p>Analysis of Variance (ANOVA) is a statistical method, used to check the means of two or more groups that are significantly different from each other. It assumes Hypothesis as</p>
<ul class="simple">
<li><p>H0: Means of all groups are equal (null hypothesis).</p></li>
<li><p>H1: At least one mean of the groups are different (alternative hypothesis).</p></li>
</ul>
<p>ANOVA uses <strong>F-test</strong> check if there is any significant difference between the groups. If there is no significant difference between the groups that all variances are equal, the result of ANOVA’s F-ratio will be close to 1.</p>
<p><strong>How comparison of means transformed to the comparison of variance?</strong>
Consider two distributions and their behavior in below fig.</p>
<center>
<img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVvwgvfkVgQjoWLrOkFjXA.png" width="500" />
</center> 
<p><a class="reference external" href="https://towardsdatascience.com/anova-for-feature-selection-in-machine-learning-d9305e228476">reference</a></p>
<p>From the above fig, we can say If the distributions overlap or close, the grand mean will be similar to individual means whereas if distributions are far, the grand mean and individual means differ by larger distance.
It refers to variations between the groups as the values in each group are different. So in ANOVA, we will compare Between-group variability to Within-group variability.</p>
<p><strong>How compute ANOVA?</strong></p>
<ul class="simple">
<li><p>Between the Sum of Squares:</p></li>
</ul>
<p>Compute the distance between each group average value g from grand means xbar. Doing similar to the total sum of squares we get</p>
<p><span class="math notranslate nohighlight">\(SSB = \sum_i(g_i-\bar X)^2\)</span></p>
<ul class="simple">
<li><p>Within the Sum of Squares:</p></li>
</ul>
<p>Compute the distance between each observed value within the group x from the group-mean g. Doing similar to the total sum of squares we get</p>
<p><span class="math notranslate nohighlight">\(SSE = \sum(x_i-g)^2\)</span></p>
<ul class="simple">
<li><p>Total Sum of Squares</p></li>
</ul>
<p>The distance between each observed point x from the grand mean xbar is x-xbar. If you calculate this distance between each data point, square each distance and add up all the squared distances you get</p>
<p><span class="math notranslate nohighlight">\(SST = \sum(x_i - \bar X)\)</span></p>
<p>The total sum of squares = SSB + SSE</p>
<ul class="simple">
<li><p>Degrees of Freedom</p></li>
</ul>
<p>Degrees of freedom refers to the maximum number of logically independent values, which have the freedom to vary. In simple words, it can be defined as the total number of observations minus the number of independent constraints imposed on the observations.</p>
<p>Df = N -1 where N is the Sample Size</p>
</section>
<section id="f-value">
<h2>F-value<a class="headerlink" href="#f-value" title="Link to this heading">#</a></h2>
<p>Since we are comparing the variance between the groups and variance within the groups. The F value is given as</p>
<p><span class="math notranslate nohighlight">\( F-value = \frac{\frac{SSB}{df_b}}{\frac{SSW}{df_w}}\)</span></p>
<ul class="simple">
<li><p>Compute the ANOVA F-value for the provided sample.</p></li>
<li><p>If the features are categorical, we will calculate a chi-square statistic between each feature and the target vector. However, if the features are quantitative, we will compute the ANOVA F-value between each feature and the target vector.</p></li>
<li><p>The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different.</p></li>
</ul>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h2>
<p>Let’s consider a school dataset having data about student’s performance. We have to predict the final grade of the student based on features like age, guardian, study time, failures, activities, etc.</p>
<p>By using One Way ANOVA let us determine is there any impact of the guardian on the final grade. Below is the data</p>
<center>
    <img src = "https://miro.medium.com/v2/resize:fit:386/format:webp/1*zkAbiPcd7uTPYOK1OTSvNg.png" width="200" />
</center>
<p>We can see guardian ( mother, father, other ) as columns and student final grade in rows.
Calculating Sum of Squares and F value here is the summary.</p>
<center>
    <img src = "https://miro.medium.com/v2/resize:fit:640/format:webp/1*JfwXOsRg7O8QFIGG3ICeuQ.png" width = "400" />
</center>
<p>With 95% confidence, alpha = 0.05 , df1 =2 ,df2 =51 given F value from the <a class="reference external" href="https://www.socscistatistics.com/tests/criticalvalues/default.aspx">F table</a> is 3.179 and the calculated F value is 18.49.</p>
<center>
    <img src = "https://miro.medium.com/v2/resize:fit:828/format:webp/1*xxfTC43OWJEXNIqshdeP9Q.png" width = "400" />
</center>
we see that the calculated F value falls in the rejection region that is beyond our confidence level. So we are rejecting the Null Hypothesis.<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_classif</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load iris data</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Create features and target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select Features With Best ANOVA F-Values</span>

<span class="c1"># Create an SelectKBest object to select features with two best ANOVA F-Values</span>
<span class="n">fvalue_selector</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Apply the SelectKBest object to the features and target</span>
<span class="n">X_kbest</span> <span class="o">=</span> <span class="n">fvalue_selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># View results</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original number of features:&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Reduced number of features:&#39;</span><span class="p">,</span> <span class="n">X_kbest</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original number of features: 4
Reduced number of features: 2
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that the above code helps us to select the 2 best features based on ANOVA F-Value.</p></li>
</ul>
</section>
<section id="correlation-matrix-with-heatmap">
<h2><strong>2.6 Correlation-Matrix with Heatmap</strong> <a class="anchor" id="2.6"></a><a class="headerlink" href="#correlation-matrix-with-heatmap" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p><strong>Correlation</strong> is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other.</p></li>
<li><p><strong>Good variables are highly correlated with the target</strong>.</p></li>
<li><p>Correlated predictor variables provide redundant information.</p></li>
<li><p><strong>Variables should be correlated with the target but uncorrelated among themselves</strong>.</p></li>
<li><p>Therefore, the Correlation Feature Selection evaluates subsets of features on the basis of the following hypothesis:</p>
<ul>
<li><p>“Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other”.</p></li>
</ul>
</li>
<li><p>In this section,  I will demonstrate how to select features based on correlation between two features. We can find features that are correlated with each other. By identifying these features, we can then decide which features we want to keep, and which ones we want to remove.</p></li>
</ul>
<ul class="simple">
<li><p>Using Pearson correlation our returned coefficient values will vary between -1 and 1.</p></li>
<li><p>If the correlation between two features is 0 this means that changing any of these two features will not affect the other.</p></li>
<li><p>If the correlation between two features is greater than 0 this means that increasing the values in one feature will make increase also the values in the other feature (the closer the correlation coefficient is to 1 and the stronger is going to be this bond between the two different features).</p></li>
<li><p>If the correlation between two features is less than 0 this means that increasing the values in one feature will make decrease the values in the other feature (the closer the correlation coefficient is to -1 and the stronger is going to be this relationship between the two different features).</p></li>
<li><p>In this analysis we will check if the selected variables are highly correlated with each other. If they are, we would then need to keep just one of the correlated ones and drop the others.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load iris data</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Create features and target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert feature matrix into DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># View the data frame</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>       0    1    2    3
0    5.1  3.5  1.4  0.2
1    4.9  3.0  1.4  0.2
2    4.7  3.2  1.3  0.2
3    4.6  3.1  1.5  0.2
4    5.0  3.6  1.4  0.2
..   ...  ...  ...  ...
145  6.7  3.0  5.2  2.3
146  6.3  2.5  5.0  1.9
147  6.5  3.0  5.2  2.0
148  6.2  3.4  5.4  2.3
149  5.9  3.0  5.1  1.8

[150 rows x 4 columns]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create correlation matrix</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>          0         1         2         3
0  1.000000 -0.117570  0.871754  0.817941
1 -0.117570  1.000000 -0.428440 -0.366126
2  0.871754 -0.428440  1.000000  0.962865
3  0.817941 -0.366126  0.962865  1.000000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create correlation heatmap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Correlation Heatmap of Iris Dataset&#39;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;.2f&#39;</span><span class="p">,</span> <span class="n">linecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>           
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/05a3d419213a15bd76b41ee047aa41e9122591856f3e9153ed94326165d026d8.png" src="../_images/05a3d419213a15bd76b41ee047aa41e9122591856f3e9153ed94326165d026d8.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span> <span class="c1"># Select upper triangle of correlation matrix</span>
<span class="n">upper</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
<span class="n">upper</span>    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>-0.11757</td>
      <td>0.871754</td>
      <td>0.817941</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>-0.428440</td>
      <td>-0.366126</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.962865</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find index of feature columns with correlation greater than 0.9</span>
<span class="n">to_drop</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">upper</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">upper</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">to_drop</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[3]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Drop Marked Features</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">to_drop</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that we have dropped the third column from the original dataset.</p></li>
</ul>
</section>
<section id="relief-based-feature-selection">
<h2><strong>2.7 Relief-based feature selection</strong> <a class="anchor" id="2.7"></a><a class="headerlink" href="#relief-based-feature-selection" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<p>Take a data set with n instances of p features, belonging to two known classes. Within the data set, each feature should be scaled to the interval [0 1] (binary data should remain as 0 and 1). The algorithm will be repeated m times. Start with a p-long weight vector (W) of zeros.</p>
<p>At each stage in this method,a sample is randomly selected from among the samples available in the dataset. Then, the relevance of each feature is updated based on the difference between the selected sample and its two nearest neighbors (one belonging to the same class as the selected sample (Hit) and the other belonging to the opposite class (Miss)). If one of the features of the selected sample differs from the corresponding feature in the neighbor sample of the same class (Hit sample), the score of this feature decreases. On the other hand, if the same feature in the selected sample differs from the corresponding feature in the neighbor sample of the opposite class (Miss sample), the score of this feature increases.</p>
<p><span class="math notranslate nohighlight">\( W_{i}=W_{i}-(x_{i}-\mathrm {nearHit} _{i})^{2}+(x_{i}-\mathrm {nearMiss} _{i})^{2}\)</span></p>
<center>
    <img src = "https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Relief_Wiki.svg/1280px-Relief_Wiki.svg.png" width = "500"/>
</center>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>sklearn_relief
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x783622786cf8&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/sklearn-relief/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x783622786588&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/sklearn-relief/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x783622786438&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/sklearn-relief/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x783622786630&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/sklearn-relief/</span>
<span class=" -Color -Color-Yellow">WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by &#39;NewConnectionError(&#39;&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x783622786390&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution&#39;,)&#39;: /simple/sklearn-relief/</span>
<span class=" -Color -Color-Red">ERROR: Could not find a version that satisfies the requirement sklearn_relief (from versions: none)</span>
<span class=" -Color -Color-Red">ERROR: No matching distribution found for sklearn_relief</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">skrebate</span> <span class="kn">import</span> <span class="n">ReliefF</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">genetic_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://github.com/EpistasisLab/scikit-rebate/raw/master/data/&#39;</span>
                           <span class="s1">&#39;GAMETES_Epistasis_2-Way_20atts_0.4H_EDM-1_1.tsv.gz&#39;</span><span class="p">,</span>
                           <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>

<span class="n">features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">genetic_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;class&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">genetic_data</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">ReliefF</span><span class="p">(</span><span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
                    <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">b64eedd197e6</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="kn">from</span> <span class="nn">skrebate</span> <span class="kn">import</span> <span class="n">ReliefF</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;skrebate&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id3">
<h1><strong>3. Wrapper Methods</strong> <a class="anchor" id="3"></a><a class="headerlink" href="#id3" title="Link to this heading">#</a></h1>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from the subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.</p></li>
<li><p>Some common examples of wrapper methods are</p>
<ul>
<li><ol class="arabic simple">
<li><p>Forward selection,</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>Backward elimination,</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>Exhaustive feature selection,</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="4">
<li><p>Recursive feature elimination.</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="5">
<li><p>Recursive feature elimination with cross-validation</p></li>
</ol>
</li>
</ul>
</li>
<li><p>Wrapper methods can be explained with the help of following graphic:</p></li>
</ul>
<center>
    <img src = "https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537549832/Image2_ajaeo8.png" width = "700" />
</center>
<p>Image source : AnalyticsVidhya</p>
<section id="forward-selection">
<h2><strong>3.1 Forward Selection</strong> <a class="anchor" id="3.1"></a><a class="headerlink" href="#forward-selection" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.</p></li>
<li><p>The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set.</p></li>
<li><p>Step forward feature selection starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria.</p></li>
<li><p>The pre-set criteria can be the roc_auc (Area under the ROC Curve) for classification and the r squared for regression for example.</p></li>
<li><p>This selection procedure is called greedy, because it evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.</p></li>
<li><p>There is a special package for python that implements this type of feature selection: mlxtend.</p></li>
<li><p>In the mlxtend implementation of the step forward feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.</p></li>
<li><p>I will demonstrate the Step Forward feature selection algorithm from mlxtend using the House Price dataset.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># step forward feature selection</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#load dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/house-prices-advanced-regression-techniques/train.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># In practice, feature selection should be done after data pre-processing,</span>
<span class="c1"># so ideally, all the categorical variables are encoded into numbers,</span>
<span class="c1"># and then you can assess how deterministic they are of the target</span>

<span class="c1"># here for simplicity I will use only numerical variables</span>
<span class="c1"># select numerical columns:</span>

<span class="n">numerics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;int16&#39;</span><span class="p">,</span> <span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="s1">&#39;int64&#39;</span><span class="p">,</span> <span class="s1">&#39;float16&#39;</span><span class="p">,</span> <span class="s1">&#39;float32&#39;</span><span class="p">,</span> <span class="s1">&#39;float64&#39;</span><span class="p">]</span>
<span class="n">numerical_vars</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="n">numerics</span><span class="p">)</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">numerical_vars</span><span class="p">]</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># separate train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;SalePrice&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;SalePrice&#39;</span><span class="p">],</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># find and remove correlated features</span>
<span class="k">def</span> <span class="nf">correlation</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="n">col_corr</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># Set of all the names of correlated columns</span>
    <span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span> <span class="c1"># we are interested in absolute coeff value</span>
                <span class="n">colname</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># getting the name of column</span>
                <span class="n">col_corr</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">colname</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">col_corr</span>

<span class="n">corr_features</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;correlated features: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">corr_features</span><span class="p">))</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># removed correlated  features</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">corr_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">corr_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># step forward feature selection</span>

<span class="kn">from</span> <span class="nn">mlxtend.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span> <span class="k">as</span> <span class="n">SFS</span>

<span class="n">sfs1</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(),</span> 
           <span class="n">k_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
           <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
           <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
           <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
           <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span>
           <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">sfs1</span> <span class="o">=</span> <span class="n">sfs1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sfs1</span><span class="o">.</span><span class="n">k_feature_idx_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs1</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that forward feature selection results in the above columns being selected from all the given columns.</p></li>
</ul>
</section>
<section id="backward-elimination">
<h2><strong>3.2 Backward Elimination</strong> <a class="anchor" id="3.2"></a><a class="headerlink" href="#backward-elimination" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.</p></li>
<li><p>The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># step backward feature elimination</span>

<span class="n">sfs1</span> <span class="o">=</span> <span class="n">SFS</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(),</span> 
           <span class="n">k_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
           <span class="n">forward</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
           <span class="n">floating</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
           <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
           <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span>
           <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">sfs1</span> <span class="o">=</span> <span class="n">sfs1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sfs1</span><span class="o">.</span><span class="n">k_feature_idx_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">sfs1</span><span class="o">.</span><span class="n">k_feature_idx_</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>So, backward feature elimination results in the following columns being selected.</p></li>
</ul>
</section>
<section id="exhaustive-feature-selection">
<h2><strong>3.3 Exhaustive Feature Selection</strong> <a class="anchor" id="3.3"></a><a class="headerlink" href="#exhaustive-feature-selection" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>In an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm. For example, if the classifier is a logistic regression and the dataset consists of 4 features, the algorithm will evaluate all 15 feature combinations as follows:</p>
<ul>
<li><p>all possible combinations of 1 feature</p></li>
<li><p>all possible combinations of 2 features</p></li>
<li><p>all possible combinations of 3 features</p></li>
<li><p>all the 4 features</p></li>
</ul>
</li>
</ul>
<p>and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.</p>
<ul class="simple">
<li><p>This is another greedy algorithm as it evaluates all possible feature combinations. It is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.</p></li>
<li><p>There is a special package for python that implements this type of feature selection: mlxtend.</p></li>
<li><p>In the mlxtend implementation of the exhaustive feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.</p></li>
<li><p>This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features.</p></li>
</ul>
</section>
<section id="recursive-feature-elimination">
<h2><strong>3.4 Recursive Feature elimination</strong> <a class="anchor" id="3.4"></a><a class="headerlink" href="#recursive-feature-elimination" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.</p></li>
<li><p>Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2N combinations of features.</p></li>
<li><p>Source : <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py">https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py</a></p></li>
</ul>
</section>
<section id="recursive-feature-elimination-with-cross-validation">
<h2><strong>3.5 Recursive Feature Elimination with Cross-Validation</strong> <a class="anchor" id="3.5"></a><a class="headerlink" href="#recursive-feature-elimination-with-cross-validation" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p><strong>Recursive Feature Elimination with Cross-Validated (RFECV)</strong> feature selection technique selects the best subset of features for the estimator by removing 0 to N features iteratively using recursive feature elimination.</p></li>
<li><p>Then it selects the best subset based on the accuracy or cross-validation score or roc-auc of the model. Recursive feature elimination technique eliminates n features from a model by fitting the model multiple times and at each step, removing the weakest features.</p></li>
<li><p>Please see my kernel - <a class="reference external" href="https://www.kaggle.com/prashant111/extensive-analysis-eda-fe-modelling">Extensive Analysis - EDA + FE + Modelling : Section 19 Recursive FeaTure Elimination with Cross-Validation</a></p></li>
<li><p>Source : <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py">https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py</a></p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id4">
<h1><strong>4. Embedded Methods</strong> <a class="anchor" id="4"></a><a class="headerlink" href="#id4" title="Link to this heading">#</a></h1>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.</p></li>
<li><p>This is why Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).</p></li>
<li><p>Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting.</p></li>
<li><p>Embedded methods can be explained with the help of following graphic:</p></li>
</ul>
<p><img alt="Embedded Methods" src="https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Embedded_1.png" /></p>
<section id="image-source-analyticsvidhya">
<h2>Image source : AnalyticsVidhya<a class="headerlink" href="#image-source-analyticsvidhya" title="Link to this heading">#</a></h2>
</section>
<section id="lasso-regression">
<h2><strong>4.1 LASSO Regression</strong><a class="anchor" id="4.1"></a><a class="headerlink" href="#lasso-regression" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.</p></li>
<li><p>Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularisation, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularisation, Lasso or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model. Then the objective of lasso is to solve</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\min _{\beta _{0},\beta }{\biggl \{}\sum _{i=1}^{N}{\bigl (}y_{i}-\beta _{0}-x_{i}^{T}\beta {\bigr )}^{2}{\biggr \}}{\text{ subject to }}\sum _{j=1}^{p}|\beta _{j}|\leq t\)</span></p>
<p>Here <span class="math notranslate nohighlight">\(\beta _{0}\)</span> is the constant coefficient, <span class="math notranslate nohighlight">\(\beta :=(\beta _{1},\beta _{2},\ldots ,\beta _{p})\)</span> is the coefficient vector, and <span class="math notranslate nohighlight">\(t\)</span> is a prespecified free parameter that determines the degree of regularization.
Letting <span class="math notranslate nohighlight">\(X\)</span> be the covariate matrix, so that <span class="math notranslate nohighlight">\(X_{ij}=(x_{i})_{j}\)</span> and <span class="math notranslate nohighlight">\( x_{i}^{T}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th row of <span class="math notranslate nohighlight">\(X\)</span>, the expression can be written more compactly as
<span class="math notranslate nohighlight">\(\min _{\beta _{0},\beta }\left\{\left\|y-\beta _{0}-X\beta \right\|_{2}^{2}\right\}{\text{ subject to }}\|\beta \|_{1}\leq t\)</span>, where <span class="math notranslate nohighlight">\(\|u\|_{p}={\biggl (}\sum _{i=1}^{N}|u_{i}|^{p}{\biggr )}^{1/p}\)</span> is the standard <span class="math notranslate nohighlight">\(\ell ^{p}\)</span> norm.</p>
<ul class="simple">
<li><p>I will demonstrate how to select features using the Lasso regularisation on the House Price dataset from Kaggle.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#load libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/house-prices-advanced-regression-techniques/train.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># In practice, feature selection should be done after data pre-processing,</span>
<span class="c1"># so ideally, all the categorical variables are encoded into numbers,</span>
<span class="c1"># and then you can assess how deterministic they are of the target</span>

<span class="c1"># here for simplicity I will use only numerical variables</span>
<span class="c1"># select numerical columns:</span>

<span class="n">numerics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;int16&#39;</span><span class="p">,</span> <span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="s1">&#39;int64&#39;</span><span class="p">,</span> <span class="s1">&#39;float16&#39;</span><span class="p">,</span> <span class="s1">&#39;float32&#39;</span><span class="p">,</span> <span class="s1">&#39;float64&#39;</span><span class="p">]</span>
<span class="n">numerical_vars</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="n">numerics</span><span class="p">)</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">numerical_vars</span><span class="p">]</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># separate train and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;SalePrice&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;SalePrice&#39;</span><span class="p">],</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the features in the house dataset are in very</span>
<span class="c1"># different scales, so it helps the regression to scale them</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># here, again I will train a Lasso Linear regression and select</span>
<span class="c1"># the non zero features in one line.</span>
<span class="c1"># bear in mind that the linear regression object from sklearn does</span>
<span class="c1"># not allow for regularisation. So If you want to make a regularised</span>
<span class="c1"># linear regression you need to import specifically &quot;Lasso&quot;</span>
<span class="c1"># that is the l1 version of the linear regression</span>
<span class="c1"># alpha is the penalisation here, so I set it high in order</span>
<span class="c1"># to force the algorithm to shrink some coefficients</span>

<span class="n">sel_</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
<span class="n">sel_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sel_</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make a list with the selected features and print the outputs</span>
<span class="n">selected_feat</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[(</span><span class="n">sel_</span><span class="o">.</span><span class="n">get_support</span><span class="p">())]</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;total features: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;selected features: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">selected_feat</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;features with coefficients shrank to zero: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sel_</span><span class="o">.</span><span class="n">estimator_</span><span class="o">.</span><span class="n">coef_</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We can see that Lasso regularisation helps to remove non-important features from the dataset. So, increasing the penalisation will result in increase the number of features removed. Therefore, we need to keep an eye and monitor that we don’t set a penalty too high so that to remove even important features, or too low and then not remove non-important features.</p></li>
<li><p>If the penalty is too high and important features are removed, we will notice a drop in the performance of the algorithm and then realise that we need to decrease the regularisation.</p></li>
</ul>
</section>
<section id="random-forest-importance">
<h2><strong>4.2 Random Forest Importance</strong><a class="anchor" id="4.2"></a><a class="headerlink" href="#random-forest-importance" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.</p></li>
<li><p>Random forests consist of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or combination of features. At each node (this is at each question), the three divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how “pure” each of the buckets is.</p></li>
<li><p>For classification, the measure of impurity is either the Gini impurity or the information gain/entropy. For regression the measure of impurity is variance. Therefore, when training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.</p></li>
<li><p>To give a better intuition, features that are selected at the top of the trees are in general more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.</p></li>
<li><p>Please see my kernel, <a class="reference external" href="https://www.kaggle.com/prashant111/random-forest-classifier-feature-importance">Random Forest Classifier + Feature Importance - Section 13. Find important features with Random Forest model</a> to know how to find important features using the random forest model.</p></li>
<li><p>I will demonstrate this process using the mushroom classification dataset as follows:-</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import libraries</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/kaggle/input/mushroom-classification/mushrooms.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Declare feature vector and target variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;class&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Encode categorical variables</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">prefix_sep</span><span class="o">=</span><span class="s1">&#39;_&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Normalize feature vector</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split the dataset</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># instantiate the classifier with n_estimators = 100</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit the classifier to the training set</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="feature-importance">
<h2><strong>Feature Importance</strong><a class="headerlink" href="#feature-importance" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importance of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it’s predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualize feature importance</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">feat_importances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">feat_importances</span><span class="o">.</span><span class="n">nlargest</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;barh&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Now we know which features are most important in the Random Forest model, we can train our model just using these features.</p></li>
<li><p>I have implemented this in the kernel - <a class="reference external" href="https://www.kaggle.com/prashant111/random-forest-classifier-feature-importance">Random Forest Classifier + Feature Importance : Section 15 - Build the Random Forest model on selected features</a>. It resulted in improved accuracy.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="how-to-choose-the-right-feature-selection-method">
<h1><strong>5. How to choose the right feature selection method</strong> <a class="anchor" id="5"></a><a class="headerlink" href="#how-to-choose-the-right-feature-selection-method" title="Link to this heading">#</a></h1>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>We can see that there are lot of feature selection techniques available.</p></li>
<li><p>The following graphic will serve as a guide on how to choose a feature selection method:-</p></li>
</ul>
<p><img alt="How to Choose a Feature Selection Method" src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png" /></p>
<p>Image Source : Machine Learning Mastery</p>
<section id="numerical-input-numerical-output">
<h2><strong>Numerical Input, Numerical Output</strong><a class="headerlink" href="#numerical-input-numerical-output" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>This is a regression predictive modeling problem with numerical input variables.</p></li>
<li><p>The most common techniques are to use a correlation coefficient, such as Pearson’s for a linear correlation, or rank-based methods for a nonlinear correlation.</p></li>
<li><p>The tests emplyed are as follows:-</p>
<ul>
<li><p>Pearson’s correlation coefficient (linear).</p></li>
<li><p>Spearman’s rank coefficient (nonlinear)</p></li>
</ul>
</li>
</ul>
</section>
<section id="numerical-input-categorical-output">
<h2><strong>Numerical Input, Categorical Output</strong><a class="headerlink" href="#numerical-input-categorical-output" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>This is a classification predictive modeling problem with numerical input variables.</p></li>
<li><p>This might be the most common example of a classification problem,</p></li>
<li><p>Again, the most common techniques are correlation based, although in this case, they must take the categorical target into account.</p></li>
<li><p>We can employ the following tests as follows:-</p>
<ul>
<li><p>ANOVA correlation coefficient (linear).</p></li>
<li><p>Kendall’s rank coefficient (nonlinear).</p></li>
</ul>
</li>
<li><p>Kendall does assume that the categorical variable is ordinal.</p></li>
</ul>
</section>
<section id="categorical-input-numerical-output">
<h2><strong>Categorical Input, Numerical Output</strong><a class="headerlink" href="#categorical-input-numerical-output" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>This is a regression predictive modeling problem with categorical input variables.</p></li>
<li><p>This is a strange example of a regression problem (e.g. we will not encounter it often).</p></li>
<li><p>We can use the same “Numerical Input, Categorical Output” methods (described above), but in reverse.</p></li>
</ul>
</section>
<section id="categorical-input-categorical-output">
<h2><strong>Categorical Input, Categorical Output</strong><a class="headerlink" href="#categorical-input-categorical-output" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>This is a classification predictive modeling problem with categorical input variables.</p></li>
<li><p>The most common correlation measure for categorical data is the chi-squared test. We can also use mutual information (information gain) from the field of information theory.</p></li>
<li><p>The following tests can be employed in this case -</p>
<ul>
<li><p>Chi-Squared test (contingency tables).</p></li>
<li><p>Mutual Information.</p></li>
</ul>
</li>
</ul>
<p>In fact, mutual information is a powerful method that may prove useful for both categorical and numerical data, e.g. it is agnostic to the data types.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="tips-and-tricks-for-feature-selection">
<h1><strong>6. Tips and Tricks for Feature Selection</strong> <a class="anchor" id="6"></a><a class="headerlink" href="#tips-and-tricks-for-feature-selection" title="Link to this heading">#</a></h1>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>In this section, we provide some additional considerations when using filter-based feature selection.</p></li>
</ul>
<section id="correlation-statistics">
<h2><strong>Correlation Statistics</strong><a class="headerlink" href="#correlation-statistics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The scikit-learn library provides an implementation of most of the useful statistical measures.</p></li>
<li><p>For example:</p>
<ul>
<li><p>Pearson’s Correlation Coefficient: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html">f_regression()</a></p></li>
<li><p>ANOVA: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html">f_classif()</a></p></li>
<li><p>Chi-Squared: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html">chi2()</a></p></li>
<li><p>Mutual Information: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html">mutual_info_classif()</a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html">mutual_info_regression()</a>.</p></li>
</ul>
</li>
<li><p>Also, the SciPy library provides an implementation of many more statistics, such as Kendall’s tau (kendalltau) and Spearman’s rank correlation (spearmanr).</p></li>
</ul>
</section>
<section id="selection-method">
<h2><strong>Selection Method</strong><a class="headerlink" href="#selection-method" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The scikit-learn library also provides many different filtering methods once statistics have been calculated for each input variable with the target.</p></li>
<li><p>Two of the more popular methods include:</p>
<ul>
<li><p>Select the top k variables: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">SelectKBest</a></p></li>
<li><p>Select the top percentile variables: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html">SelectPercentile</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="transform-variables">
<h2><strong>Transform Variables</strong><a class="headerlink" href="#transform-variables" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>We can consider transforming the variables in order to access different statistical methods. For example, we can transform a categorical variable to ordinal, even if it is not, and see if any interesting results come out.</p></li>
<li><p>We can also make a numerical variable discrete (e.g. bins); try categorical-based measures.</p></li>
<li><p>Some statistical measures assume properties of the variables, such as Pearson’s that assumes a Gaussian probability distribution to the observations and a linear relationship. You can transform the data to meet the expectations of the test and try the test regardless of the expectations and compare results.</p></li>
</ul>
</section>
<section id="what-is-the-best-method">
<h2><strong>What Is the Best Method?</strong><a class="headerlink" href="#what-is-the-best-method" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>There is no best feature selection method. Just like there is no best set of input variables or best machine learning algorithm.</p></li>
<li><p>Instead, we must discover what works best for your specific problem using careful systematic experimentation.</p></li>
<li><p>We should try a range of different models fit on different subsets of features chosen via different statistical measures and discover what works best for your specific problem.</p></li>
</ul>
</section>
<section id="best-ways-of-feature-selection">
<h2><strong>4 best ways of Feature Selection</strong><a class="headerlink" href="#best-ways-of-feature-selection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The 4 practical ways of feature selection which yield best results are as follows:-</p>
<ol class="arabic simple">
<li><p>SelectKBest</p></li>
<li><p>Recursive Feature Elimination</p></li>
<li><p>Correlation-matrix with heatmap</p></li>
<li><p>Random-Forest Importance</p></li>
</ol>
</li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="references">
<h1><strong>7. References</strong> <a class="anchor" id="7"></a><a class="headerlink" href="#references" title="Link to this heading">#</a></h1>
<p><a class="reference internal" href="#0.1"><span class="xref myst">Table of Contents</span></a></p>
<ul class="simple">
<li><p>The work done in this kernel is inspired from the following courses and websites:-</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.udemy.com/course/feature-selection-for-machine-learning/">Feature Selection for Machine Learning</a> by Soledad Galli</p></li>
<li><p><a class="reference external" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/">Analytics Vidhya article on Feature Selection</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Feature_selection">https://en.wikipedia.org/wiki/Feature_selection</a></p></li>
<li><p><a class="reference external" href="https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/">https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/</a></p></li>
</ol>
</li>
</ul>
<p><a class="reference internal" href="#0"><span class="xref myst">Go to Top</span></a></p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./_sources"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#"><strong>Comprehensive Guide on Feature Selection</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-feature-selection"><strong>1. Introduction to Feature Selection</strong> <a class="anchor" id="1"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection"><strong>Feature selection</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-selecting-features"><strong>Advantages of selecting features</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-techniques"><strong>Feature Selection – Techniques</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-source-miro-medium-com">Image source : miro.medium.com</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#filter-methods"><strong>Filter Methods</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapper-methods"><strong>Wrapper Methods</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#embedded-methods"><strong>Embedded Methods</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>2. Filter Methods</strong> <a class="anchor" id="2"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-methods"><strong>2.1 Basic methods</strong> <a class="anchor" id="2.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-constant-features"><strong>2.1.1 Remove constant features</strong> <a class="anchor" id="2.1.1"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important"><strong>Important</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-variance-threshold-from-sklearn"><strong>Using variance threshold from sklearn</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-quasi-constant-features"><strong>2.1.2 Remove quasi-constant features</strong> <a class="anchor" id="2.1.2"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-source-towardsdatascience">Image source : towardsdatascience</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#removing-quasi-constant-features"><strong>Removing quasi-constant features</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Using variance threshold from sklearn</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-selection-methods"><strong>2.2 Univariate selection methods</strong> <a class="anchor" id="2.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selectkbest"><strong>2.2.1 SelectKBest</strong> <a class="anchor" id="2.2.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selectpercentile"><strong>2.2.2 SelectPercentile</strong> <a class="anchor" id="2.2.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#important-information"><strong>Important information</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-with-sparse-data"><strong>Feature selection with sparse data</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warning"><strong>Warning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#information-gain"><strong>2.3 Information Gain</strong> <a class="anchor" id="2.3"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-info-classif"><strong>mutual_info_classif</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mutual-info-regression"><strong>mutual_info_regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fisher-score-chi-square-implementation"><strong>2.4 Fisher Score (chi-square implementation)</strong> <a class="anchor" id="2.4"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#anova-f-value-for-feature-selection"><strong>2.5 ANOVA F-value For Feature Selection</strong> <a class="anchor" id="2.5"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f-value">F-value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-matrix-with-heatmap"><strong>2.6 Correlation-Matrix with Heatmap</strong> <a class="anchor" id="2.6"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relief-based-feature-selection"><strong>2.7 Relief-based feature selection</strong> <a class="anchor" id="2.7"></a></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>3. Wrapper Methods</strong> <a class="anchor" id="3"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-selection"><strong>3.1 Forward Selection</strong> <a class="anchor" id="3.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-elimination"><strong>3.2 Backward Elimination</strong> <a class="anchor" id="3.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exhaustive-feature-selection"><strong>3.3 Exhaustive Feature Selection</strong> <a class="anchor" id="3.3"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination"><strong>3.4 Recursive Feature elimination</strong> <a class="anchor" id="3.4"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination-with-cross-validation"><strong>3.5 Recursive Feature Elimination with Cross-Validation</strong> <a class="anchor" id="3.5"></a></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>4. Embedded Methods</strong> <a class="anchor" id="4"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-source-analyticsvidhya">Image source : AnalyticsVidhya</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression"><strong>4.1 LASSO Regression</strong><a class="anchor" id="4.1"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-importance"><strong>4.2 Random Forest Importance</strong><a class="anchor" id="4.2"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance"><strong>Feature Importance</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-choose-the-right-feature-selection-method"><strong>5. How to choose the right feature selection method</strong> <a class="anchor" id="5"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-input-numerical-output"><strong>Numerical Input, Numerical Output</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-input-categorical-output"><strong>Numerical Input, Categorical Output</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-input-numerical-output"><strong>Categorical Input, Numerical Output</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-input-categorical-output"><strong>Categorical Input, Categorical Output</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#tips-and-tricks-for-feature-selection"><strong>6. Tips and Tricks for Feature Selection</strong> <a class="anchor" id="6"></a></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-statistics"><strong>Correlation Statistics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-method"><strong>Selection Method</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transform-variables"><strong>Transform Variables</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-best-method"><strong>What Is the Best Method?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-ways-of-feature-selection"><strong>4 best ways of Feature Selection</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references"><strong>7. References</strong> <a class="anchor" id="7"></a></a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Soheila Ashkezari-T.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>