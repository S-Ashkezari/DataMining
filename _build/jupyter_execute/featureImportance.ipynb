{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cae75ed-da7d-4a0c-8e07-75c30d0a078f",
   "metadata": {},
   "source": [
    "### featureImportance.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7c233-6c0f-45f9-9699-e3f4d383f327",
   "metadata": {},
   "source": [
    "## Introduction to the Importance of Features in Data Mining  \n",
    "\n",
    "### The Role of Features in Modeling and Prediction  \n",
    "\n",
    "#### Explanation  \n",
    "Features, or attributes, are the measurable properties or characteristics of the data that play a crucial role in data mining. They serve as the foundation for building models and making predictions. The quality and relevance of the features directly influence the accuracy, interpretability, and generalization of machine learning models.\n",
    "\n",
    "- **High-quality features**: Help models capture patterns and relationships within the data.  \n",
    "- **Irrelevant or noisy features**: May degrade model performance, leading to overfitting or misinterpretation of results.  \n",
    "\n",
    "#### Key Points:  \n",
    "1. **Features represent the data**: They encode the information required for models to learn. For instance, in a housing price prediction problem, features like the size of the house, number of bedrooms, and location are critical.  \n",
    "2. **Features impact performance**: Poorly selected features may reduce accuracy, increase training time, and make models more complex.  \n",
    "3. **Domain knowledge is essential**: Understanding the domain helps identify which features are relevant and which are not.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Example: The Role of Features in Housing Price Prediction  \n",
    "Features for this problem could include:  \n",
    "- Size (e.g., square footage).  \n",
    "- Location (e.g., distance from the city center).  \n",
    "- Year built (e.g., age of the house).  \n",
    "- Number of bedrooms and bathrooms.  \n",
    "\n",
    "**Scenario**:  \n",
    "- Without considering location, the model may fail to capture price variations due to proximity to important facilities.  \n",
    "- Including irrelevant features like the seller's favorite color might add noise and reduce model accuracy.  \n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077988fb-6dc3-4c29-8367-4a27457d9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc: 0.5249\n",
      "HouseAge: 0.0546\n",
      "AveRooms: 0.0443\n",
      "AveBedrms: 0.0296\n",
      "Population: 0.0306\n",
      "AveOccup: 0.1384\n",
      "Latitude: 0.0889\n",
      "Longitude: 0.0886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load housing data\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = model.feature_importances_\n",
    "for feature, importance in zip(data.feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd73e25-3354-4668-be3f-ed7408821c62",
   "metadata": {},
   "source": [
    "## The Difference Between Feature Selection and Feature Extraction  \n",
    "\n",
    "### Feature Selection  \n",
    "Feature selection is the process of selecting a subset of the most relevant features from the original dataset. It aims to reduce dimensionality while retaining important information.  \n",
    "\n",
    "#### Advantages:  \n",
    "- Reduces overfitting by removing irrelevant features.  \n",
    "- Decreases computational cost and complexity.  \n",
    "- Improves model interpretability.  \n",
    "\n",
    "#### Methods:  \n",
    "1. **Filter methods**: Select features based on statistical tests (e.g., correlation, chi-square).  \n",
    "2. **Wrapper methods**: Use predictive models to evaluate feature subsets (e.g., forward selection, backward elimination).  \n",
    "3. **Embedded methods**: Perform feature selection as part of model training (e.g., Lasso regression, tree-based methods).  \n",
    "\n",
    "---\n",
    "\n",
    "### Feature Extraction  \n",
    "Feature extraction transforms the original features into a new space of reduced dimensions. It combines or reformulates features to create new, more informative representations.  \n",
    "\n",
    "#### Advantages:  \n",
    "- Captures hidden patterns and interactions in the data.  \n",
    "- Useful for high-dimensional data where features are correlated or redundant.  \n",
    "- Often results in lower-dimensional representations suitable for visualization or modeling.  \n",
    "\n",
    "#### Methods:  \n",
    "1. **Linear methods**: Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA).  \n",
    "2. **Nonlinear methods**: Independent Component Analysis (ICA), t-SNE, Autoencoders.  \n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table  \n",
    "\n",
    "| Aspect                   | Feature Selection                     | Feature Extraction                  |  \n",
    "|--------------------------|---------------------------------------|-------------------------------------|  \n",
    "| Goal                     | Select a subset of existing features | Create new features                 |  \n",
    "| Dimensionality Reduction | Yes                                  | Yes                                 |  \n",
    "| Interpretability         | High (original features retained)    | Low (transformed features)          |  \n",
    "| Methods                  | Filter, Wrapper, Embedded            | PCA, ICA, Autoencoders              |  \n",
    "\n",
    "---\n",
    "\n",
    "### Example: Comparing Feature Selection and Extraction  \n",
    "\n",
    "#### Feature Selection  \n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97364019-9c4e-4f41-9bc2-cd1eb0274cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features Shape: (20640, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Select top 3 features\n",
    "selector = SelectKBest(score_func=f_regression, k=3)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "print(\"Selected Features Shape:\", X_selected.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0177dd-daf0-4169-831f-f676fa8d944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features Shape: (20640, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 3 components\n",
    "pca = PCA(n_components=3)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "print(\"Extracted Features Shape:\", X_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeed05e-e0c4-428c-b8ab-7a8e8b3be03b",
   "metadata": {},
   "source": [
    "Visualization:\n",
    "Feature Selection: Retains meaningful features for model interpretability.\n",
    "\n",
    "Feature Extraction: Creates new representations that often improve model performance in high-dimensional datasets.\n",
    "\n",
    "Understanding the difference between these techniques helps in choosing the right approach based on the problem at hand and the dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be6e03-babe-4696-88bd-aacc6f38fcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}